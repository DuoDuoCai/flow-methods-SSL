{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e557697",
   "metadata": {},
   "source": [
    "THe code regarding gradient descent and loss computation is here:\n",
    "```\n",
    "def step(self, minibatch):\n",
    "    self.optimizer.zero_grad()\n",
    "    loss = self.loss(minibatch)\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss\n",
    "    \n",
    "flow_loss = self.model.nll(x_lab,y_lab).mean() + a*self.model.nll(x_unlab).mean()\n",
    "```\n",
    "where self refers to the trainer. x_lab is the 768-dim embedding of pieces of texts, y_lab are their labels. But what is self.model? it is defined by:\n",
    "```\n",
    "model = network(num_classes=datasets['train'].num_classes,dim_in=datasets['train'].dim,**net_config).to(device)\n",
    "```\n",
    "Where \"network\" is of class flow_ssl.realnvp.realnvp.RealNVPTabular, here is the definition for the class \"RealNVPTabular\":\n",
    "```\n",
    "class RealNVPBase(nn.Module):\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.body(x)\n",
    "\n",
    "    def logdet(self):\n",
    "        return self.body.logdet()\n",
    "\n",
    "    def inverse(self,z):\n",
    "        return self.body.inverse(z)\n",
    "    \n",
    "    def nll(self,x,y=None,label_weight=1.):\n",
    "        z = self(x)\n",
    "        logdet = self.logdet()\n",
    "        z = z.reshape((z.shape[0], -1))\n",
    "        prior_ll = self.prior.log_prob(z, y,label_weight=label_weight)\n",
    "        nll = -(prior_ll + logdet)\n",
    "        return nll\n",
    "\n",
    "\n",
    "class RealNVPTabular(RealNVPBase):\n",
    "\n",
    "    def __init__(self, in_dim=2, num_coupling_layers=6, hidden_dim=256, \n",
    "                 num_layers=2, init_zeros=False,dropout=False):\n",
    "\n",
    "        super(RealNVPTabular, self).__init__()\n",
    "        \n",
    "        self.body = iSequential(*[\n",
    "                        CouplingLayerTabular(in_dim, hidden_dim, num_layers, MaskTabular(reverse_mask=bool(i%2)),\n",
    "                            init_zeros=init_zeros,dropout=dropout)\n",
    "                        for i in range(num_coupling_layers)\n",
    "                    ])\n",
    "```\n",
    "\n",
    "Here is the definition of iSequential class, which is defined in parts.py inside flow_ssl/invertible:\n",
    "\n",
    "```\n",
    "class iSequential(torch.nn.Sequential):\n",
    "\n",
    "    def inverse(self,y):\n",
    "        for module in reversed(self._modules.values()):\n",
    "            assert hasattr(module,'inverse'), '{} has no inverse defined'.format(module)\n",
    "            y = module.inverse(y)\n",
    "        return y\n",
    "\n",
    "    def logdet(self):\n",
    "        log_det = 0\n",
    "        for module in self._modules.values():\n",
    "            assert hasattr(module,'logdet'), '{} has no logdet defined'.format(module)\n",
    "            log_det += module.logdet()\n",
    "        return log_det\n",
    "\n",
    "    def reduce_func_singular_values(self,func):\n",
    "        val = 0\n",
    "        for module in self._modules.values():\n",
    "            if hasattr(module,'reduce_func_singular_values'):\n",
    "                val += module.reduce_func_singular_values(func)\n",
    "        return val\n",
    "```\n",
    "```\n",
    "class CouplingLayer(CouplingLayerBase):\n",
    "    \"\"\"Coupling layer in RealNVP for image data.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input.\n",
    "        mid_channels (int): Number of channels in the `s` and `t` network.\n",
    "        num_blocks (int): Number of residual blocks in the `s` and `t` network.\n",
    "        mask (MaskChannelWise or MaskChannelWise): mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, num_blocks, mask, init_zeros=False):\n",
    "        super(CouplingLayer, self).__init__()\n",
    "\n",
    "        self.mask = mask\n",
    "\n",
    "        # Build scale and translate network\n",
    "        if self.mask.type == MaskType.CHANNEL_WISE:\n",
    "            in_channels //= 2\n",
    "\n",
    "        # Pavel: reuse Marc's ResNet block?\n",
    "        self.st_net = ResNet(in_channels, mid_channels, 2 * in_channels,\n",
    "                             num_blocks=num_blocks, kernel_size=3, padding=1,\n",
    "                             double_after_norm=(self.mask.type == MaskType.CHECKERBOARD),\n",
    "                             init_zeros=init_zeros)\n",
    "\n",
    "        # Learnable scale for s\n",
    "        self.rescale = nn.utils.weight_norm(Rescale(in_channels))\n",
    "\n",
    "\n",
    "class CouplingLayerTabular(CouplingLayerBase):\n",
    "\n",
    "    def __init__(self, in_dim, mid_dim, num_layers, mask, init_zeros=False,dropout=False):\n",
    "\n",
    "        super(CouplingLayerTabular, self).__init__()\n",
    "        self.mask = mask\n",
    "        self.layers = [\n",
    "            nn.Linear(in_dim, mid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5) if dropout else nn.Sequential(),\n",
    "            *self._inner_seq(num_layers, mid_dim),\n",
    "        ]\n",
    "        last_layer = nn.Linear(mid_dim, in_dim*2)\n",
    "        if init_zeros:\n",
    "            nn.init.zeros_(last_layer.weight)\n",
    "            nn.init.zeros_(last_layer.bias)\n",
    "        self.layers.append(last_layer)\n",
    "\n",
    "        self.st_net = nn.Sequential(*self.layers)\n",
    "        self.rescale = nn.utils.weight_norm(RescaleTabular(in_dim))\n",
    "       \n",
    "    @staticmethod\n",
    "    def _inner_seq(num_layers, mid_dim):\n",
    "        res = []\n",
    "        for _ in range(num_layers):\n",
    "            res.append(nn.Linear(mid_dim, mid_dim))\n",
    "            res.append(nn.ReLU())\n",
    "        return res\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9381a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan, nan, nan], grad_fn=<WhereBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogDet(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.logdet(x)  # Input shape: [..., N, N]\n",
    "\n",
    "# Example: Input (4D) → Linear → Reshape → LogDet\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 4),       # Output 4 values\n",
    "    Reshape((2, 2)),       # Reshape to 2x2\n",
    "    LogDet()               # Output: [3] (logdet for each sample)\n",
    ")\n",
    "\n",
    "# Test\n",
    "x = torch.ones(3, 4)\n",
    "output = model(x)          # Shape: [3] (scalar per sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.utils import torchutils\n",
    "\n",
    "class Transform(nn.Module):\n",
    "    \"\"\"Base class for all transform objects.\"\"\"\n",
    "\n",
    "    def forward(self, inputs, context=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def inverse(self, inputs, context=None):\n",
    "        raise InverseNotAvailable()\n",
    "    \n",
    "\n",
    "class CouplingTransform(Transform):\n",
    "    \"\"\"A base class for coupling layers. Supports 2D inputs (NxD), as well as 4D inputs for\n",
    "    images (NxCxHxW). For images the splitting is done on the channel dimension, using the\n",
    "    provided 1D mask.\"\"\"\n",
    "\n",
    "    def __init__(self, mask, transform_net_create_fn, unconditional_transform=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Args:\n",
    "            mask: a 1-dim tensor, tuple or list. It indexes inputs as follows:\n",
    "                * If `mask[i] > 0`, `input[i]` will be transformed.\n",
    "                * If `mask[i] <= 0`, `input[i]` will be passed unchanged.\n",
    "        \"\"\"\n",
    "        mask = torch.as_tensor(mask)\n",
    "        if mask.dim() != 1:\n",
    "            raise ValueError(\"Mask must be a 1-dim tensor.\")\n",
    "        if mask.numel() <= 0:\n",
    "            raise ValueError(\"Mask can't be empty.\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.features = len(mask)\n",
    "        features_vector = torch.arange(self.features)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"identity_features\", features_vector.masked_select(mask <= 0)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"transform_features\", features_vector.masked_select(mask > 0)\n",
    "        )\n",
    "\n",
    "        assert self.num_identity_features + self.num_transform_features == self.features\n",
    "\n",
    "        self.transform_net = transform_net_create_fn(\n",
    "            self.num_identity_features,\n",
    "            self.num_transform_features * self._transform_dim_multiplier(),\n",
    "        )\n",
    "\n",
    "        if unconditional_transform is None:\n",
    "            self.unconditional_transform = None\n",
    "        else:\n",
    "            self.unconditional_transform = unconditional_transform(\n",
    "                features=self.num_identity_features\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def num_identity_features(self):\n",
    "        return len(self.identity_features)\n",
    "\n",
    "    @property\n",
    "    def num_transform_features(self):\n",
    "        return len(self.transform_features)\n",
    "\n",
    "    def forward(self, inputs, context=None):\n",
    "        if inputs.dim() not in [2, 4]:\n",
    "            raise ValueError(\"Inputs must be a 2D or a 4D tensor.\")\n",
    "\n",
    "        if inputs.shape[1] != self.features:\n",
    "            raise ValueError(\n",
    "                \"Expected features = {}, got {}.\".format(self.features, inputs.shape[1])\n",
    "            )\n",
    "\n",
    "        identity_split = inputs[:, self.identity_features, ...]\n",
    "        transform_split = inputs[:, self.transform_features, ...]\n",
    "\n",
    "        transform_params = self.transform_net(identity_split, context)\n",
    "        transform_split, logabsdet = self._coupling_transform_forward(\n",
    "            inputs=transform_split, transform_params=transform_params\n",
    "        )\n",
    "\n",
    "        if self.unconditional_transform is not None:\n",
    "            identity_split, logabsdet_identity = self.unconditional_transform(\n",
    "                identity_split, context\n",
    "            )\n",
    "            logabsdet += logabsdet_identity\n",
    "\n",
    "        outputs = torch.empty_like(inputs)\n",
    "        outputs[:, self.identity_features, ...] = identity_split\n",
    "        outputs[:, self.transform_features, ...] = transform_split\n",
    "\n",
    "        return outputs, logabsdet\n",
    "\n",
    "    def inverse(self, inputs, context=None):\n",
    "        if inputs.dim() not in [2, 4]:\n",
    "            raise ValueError(\"Inputs must be a 2D or a 4D tensor.\")\n",
    "\n",
    "        if inputs.shape[1] != self.features:\n",
    "            raise ValueError(\n",
    "                \"Expected features = {}, got {}.\".format(self.features, inputs.shape[1])\n",
    "            )\n",
    "\n",
    "        identity_split = inputs[:, self.identity_features, ...]\n",
    "        transform_split = inputs[:, self.transform_features, ...]\n",
    "\n",
    "        logabsdet = 0.0\n",
    "        if self.unconditional_transform is not None:\n",
    "            identity_split, logabsdet = self.unconditional_transform.inverse(\n",
    "                identity_split, context\n",
    "            )\n",
    "\n",
    "        transform_params = self.transform_net(identity_split, context)\n",
    "        transform_split, logabsdet_split = self._coupling_transform_inverse(\n",
    "            inputs=transform_split, transform_params=transform_params\n",
    "        )\n",
    "        logabsdet += logabsdet_split\n",
    "\n",
    "        outputs = torch.empty_like(inputs)\n",
    "        outputs[:, self.identity_features] = identity_split\n",
    "        outputs[:, self.transform_features] = transform_split\n",
    "\n",
    "        return outputs, logabsdet\n",
    "\n",
    "    def _transform_dim_multiplier(self):\n",
    "        \"\"\"Number of features to output for each transform dimension.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _coupling_transform_forward(self, inputs, transform_params):\n",
    "        \"\"\"Forward pass of the coupling transform.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _coupling_transform_inverse(self, inputs, transform_params):\n",
    "        \"\"\"Inverse of the coupling transform.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "class PiecewiseCouplingTransform(CouplingTransform):\n",
    "    def _coupling_transform_forward(self, inputs, transform_params):\n",
    "        return self._coupling_transform(inputs, transform_params, inverse=False)\n",
    "\n",
    "    def _coupling_transform_inverse(self, inputs, transform_params):\n",
    "        return self._coupling_transform(inputs, transform_params, inverse=True)\n",
    "\n",
    "    def _coupling_transform(self, inputs, transform_params, inverse=False):\n",
    "        if inputs.dim() == 4:\n",
    "            b, c, h, w = inputs.shape\n",
    "            # For images, reshape transform_params from Bx(C*?)xHxW to BxCxHxWx?\n",
    "            transform_params = transform_params.reshape(b, c, -1, h, w).permute(\n",
    "                0, 1, 3, 4, 2\n",
    "            )\n",
    "        elif inputs.dim() == 2:\n",
    "            b, d = inputs.shape\n",
    "            # For 2D data, reshape transform_params from Bx(D*?) to BxDx?\n",
    "            transform_params = transform_params.reshape(b, d, -1)\n",
    "\n",
    "        outputs, logabsdet = self._piecewise_cdf(inputs, transform_params, inverse)\n",
    "\n",
    "        return outputs, torchutils.sum_except_batch(logabsdet)\n",
    "\n",
    "    def _piecewise_cdf(self, inputs, transform_params, inverse=False):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "def _share_across_batch(params, batch_size):\n",
    "    return params[None, ...].expand(batch_size, *params.shape)\n",
    "\n",
    "class PiecewiseRationalQuadraticCDF(Transform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        shape,\n",
    "        num_bins=10,\n",
    "        tails=None,\n",
    "        tail_bound=1.0,\n",
    "        identity_init=False,\n",
    "        min_bin_width=splines.rational_quadratic.DEFAULT_MIN_BIN_WIDTH,\n",
    "        min_bin_height=splines.rational_quadratic.DEFAULT_MIN_BIN_HEIGHT,\n",
    "        min_derivative=splines.rational_quadratic.DEFAULT_MIN_DERIVATIVE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.min_bin_width = min_bin_width\n",
    "        self.min_bin_height = min_bin_height\n",
    "        self.min_derivative = min_derivative\n",
    "\n",
    "        self.tail_bound = tail_bound\n",
    "        self.tails = tails\n",
    "\n",
    "        if isinstance(shape, int):\n",
    "            shape = (shape,)\n",
    "        if identity_init:\n",
    "            self.unnormalized_widths = nn.Parameter(torch.zeros(*shape, num_bins))\n",
    "            self.unnormalized_heights = nn.Parameter(torch.zeros(*shape, num_bins))\n",
    "\n",
    "            constant = np.log(np.exp(1 - min_derivative) - 1)\n",
    "            num_derivatives = (\n",
    "                (num_bins - 1) if self.tails == \"linear\" else (num_bins + 1)\n",
    "            )\n",
    "            self.unnormalized_derivatives = nn.Parameter(\n",
    "                constant * torch.ones(*shape, num_derivatives)\n",
    "            )\n",
    "        else:\n",
    "            self.unnormalized_widths = nn.Parameter(torch.rand(*shape, num_bins))\n",
    "            self.unnormalized_heights = nn.Parameter(torch.rand(*shape, num_bins))\n",
    "\n",
    "            num_derivatives = (\n",
    "                (num_bins - 1) if self.tails == \"linear\" else (num_bins + 1)\n",
    "            )\n",
    "            self.unnormalized_derivatives = nn.Parameter(\n",
    "                torch.rand(*shape, num_derivatives)\n",
    "            )\n",
    "\n",
    "    def _spline(self, inputs, inverse=False):\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        unnormalized_widths = _share_across_batch(self.unnormalized_widths, batch_size)\n",
    "        unnormalized_heights = _share_across_batch(\n",
    "            self.unnormalized_heights, batch_size\n",
    "        )\n",
    "        unnormalized_derivatives = _share_across_batch(\n",
    "            self.unnormalized_derivatives, batch_size\n",
    "        )\n",
    "\n",
    "        if self.tails is None:\n",
    "            spline_fn = splines.rational_quadratic_spline\n",
    "            spline_kwargs = {}\n",
    "        else:\n",
    "            spline_fn = splines.unconstrained_rational_quadratic_spline\n",
    "            spline_kwargs = {\"tails\": self.tails, \"tail_bound\": self.tail_bound}\n",
    "\n",
    "        outputs, logabsdet = spline_fn(\n",
    "            inputs=inputs,\n",
    "            unnormalized_widths=unnormalized_widths,\n",
    "            unnormalized_heights=unnormalized_heights,\n",
    "            unnormalized_derivatives=unnormalized_derivatives,\n",
    "            inverse=inverse,\n",
    "            min_bin_width=self.min_bin_width,\n",
    "            min_bin_height=self.min_bin_height,\n",
    "            min_derivative=self.min_derivative,\n",
    "            **spline_kwargs\n",
    "        )\n",
    "\n",
    "        return outputs, torchutils.sum_except_batch(logabsdet)\n",
    "\n",
    "    def forward(self, inputs, context=None):\n",
    "        return self._spline(inputs, inverse=False)\n",
    "\n",
    "    def inverse(self, inputs, context=None):\n",
    "        return self._spline(inputs, inverse=True)\n",
    "\n",
    "\n",
    "from nflows.transforms import splines\n",
    "class PiecewiseRationalQuadraticCouplingTransform(PiecewiseCouplingTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mask,\n",
    "        transform_net_create_fn,\n",
    "        num_bins=10,\n",
    "        tails=None,\n",
    "        tail_bound=1.0,\n",
    "        apply_unconditional_transform=False,\n",
    "        img_shape=None,\n",
    "        min_bin_width=splines.rational_quadratic.DEFAULT_MIN_BIN_WIDTH,\n",
    "        min_bin_height=splines.rational_quadratic.DEFAULT_MIN_BIN_HEIGHT,\n",
    "        min_derivative=splines.rational_quadratic.DEFAULT_MIN_DERIVATIVE,\n",
    "    ):\n",
    "\n",
    "        self.num_bins = num_bins\n",
    "        self.min_bin_width = min_bin_width\n",
    "        self.min_bin_height = min_bin_height\n",
    "        self.min_derivative = min_derivative\n",
    "        self.tails = tails\n",
    "        self.tail_bound = tail_bound\n",
    "\n",
    "        if apply_unconditional_transform:\n",
    "            unconditional_transform = lambda features: PiecewiseRationalQuadraticCDF(\n",
    "                shape=[features] + (img_shape if img_shape else []),\n",
    "                num_bins=num_bins,\n",
    "                tails=tails,\n",
    "                tail_bound=tail_bound,\n",
    "                min_bin_width=min_bin_width,\n",
    "                min_bin_height=min_bin_height,\n",
    "                min_derivative=min_derivative,\n",
    "            )\n",
    "        else:\n",
    "            unconditional_transform = None\n",
    "\n",
    "        super().__init__(\n",
    "            mask,\n",
    "            transform_net_create_fn,\n",
    "            unconditional_transform=unconditional_transform,\n",
    "        )\n",
    "\n",
    "    def _transform_dim_multiplier(self):\n",
    "        if self.tails == \"linear\":\n",
    "            return self.num_bins * 3 - 1\n",
    "        else:\n",
    "            return self.num_bins * 3 + 1\n",
    "\n",
    "    def _piecewise_cdf(self, inputs, transform_params, inverse=False):\n",
    "        unnormalized_widths = transform_params[..., : self.num_bins]\n",
    "        unnormalized_heights = transform_params[..., self.num_bins : 2 * self.num_bins]\n",
    "        unnormalized_derivatives = transform_params[..., 2 * self.num_bins :]\n",
    "\n",
    "        if hasattr(self.transform_net, \"hidden_features\"):\n",
    "            unnormalized_widths /= np.sqrt(self.transform_net.hidden_features)\n",
    "            unnormalized_heights /= np.sqrt(self.transform_net.hidden_features)\n",
    "        elif hasattr(self.transform_net, \"hidden_channels\"):\n",
    "            unnormalized_widths /= np.sqrt(self.transform_net.hidden_channels)\n",
    "            unnormalized_heights /= np.sqrt(self.transform_net.hidden_channels)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"Inputs to the softmax are not scaled down: initialization might be bad.\"\n",
    "            )\n",
    "\n",
    "        if self.tails is None:\n",
    "            spline_fn = splines.rational_quadratic_spline\n",
    "            spline_kwargs = {}\n",
    "        else:\n",
    "            spline_fn = splines.unconstrained_rational_quadratic_spline\n",
    "            spline_kwargs = {\"tails\": self.tails, \"tail_bound\": self.tail_bound}\n",
    "\n",
    "        return spline_fn(\n",
    "            inputs=inputs,\n",
    "            unnormalized_widths=unnormalized_widths,\n",
    "            unnormalized_heights=unnormalized_heights,\n",
    "            unnormalized_derivatives=unnormalized_derivatives,\n",
    "            inverse=inverse,\n",
    "            min_bin_width=self.min_bin_width,\n",
    "            min_bin_height=self.min_bin_height,\n",
    "            min_derivative=self.min_derivative,\n",
    "            **spline_kwargs\n",
    "        )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
